docker-compose.yml- version: '3.8'

services:

 ollama_llama:
  image: ollama/ollama
  container_name: ollama_llama
  networks:
   - my_network
  ports:
   - "11434:11434"
  deploy:
   resources:
    reservations:
     devices:
     - driver: nvidia
      capabilities: ["gpu"]
      count: all
  volumes:
   - ollama:/root/.ollama
  restart: always

 eysip_test:
  build:
   context: .
   dockerfile: Dockerfile
  image: eysip_test # Specify the desired image name here
  container_name: eysip_test
  networks:
   - my_network
  ports:
   - "8000:8000"
  depends_on:
   - ollama_llama
  entrypoint: ["/bin/bash", "-c"]
  command: ["cd /app/inference && chainlit run app.py -w"]
  networks:
   - my_network

networks:
 my_network:
  driver: bridge

volumes:
 ollama:  
